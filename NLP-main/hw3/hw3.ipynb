{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPebTDvLa3kDkABTDwcEAtU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, SimpleRNN, Bidirectional, Dropout\n","from tensorflow.keras.models import Sequential\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import nltk # Import the main nltk library\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","from nltk.stem import WordNetLemmatizer\n","import re"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GnERNtT-nyV_","executionInfo":{"status":"ok","timestamp":1720448169707,"user_tz":-180,"elapsed":15211,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"a8991594-4bfd-4f15-8eac-d9f71b32e327"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"code","source":["from bs4 import BeautifulSoup\n","import requests"],"metadata":{"id":"M0y3ZYdC1u1T","executionInfo":{"status":"ok","timestamp":1720453214190,"user_tz":-180,"elapsed":909,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Define the URL of the Wikipedia page for LaPerm\n","url = \"https://en.wikipedia.org/wiki/LaPerm\"\n","\n","# Send a GET request to the URL\n","response = requests.get(url)\n","\n","# Parse the HTML content of the page\n","soup = BeautifulSoup(response.content, 'html.parser')\n","\n","# Find the container element that holds the main content of the page\n","content_div = soup.find('div', {'class': 'mw-parser-output'})\n","\n","# Find all <p> tags within the content area\n","if content_div:\n","    paragraphs = content_div.find_all('p')\n","\n","    # Extract text from each paragraph and concatenate them into a single string\n","    text_data = ''\n","    for paragraph in paragraphs:\n","        text_data += paragraph.get_text()\n","\n","    # Print the scraped text data\n","    print(text_data)\n","    all_words = text_data.split()\n","else:\n","    print(\"Main content container not found.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p811Zu1C1l4K","executionInfo":{"status":"ok","timestamp":1720453214730,"user_tz":-180,"elapsed":541,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"7acb06f6-53ad-45d9-8f50-0b89a9c3b126"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["The LaPerm is a breed of cat. A LaPerm's fur is curly (hence the name \"perm\"), with the tightest curls being on the throat and on the base of the ears. LaPerms come in many colors and patterns. LaPerms generally have a very affectionate personality.\n","The LaPerm is a rex breed which originated in the United States and is now present in many other countries worldwide. The breed is genetically unique and not related to any other rex cat varieties, having a dominant gene causing their curly coats. They have an elegant and athletic build and are affectionate, active, and outgoing in character. They are reputed to be hypoallergenic cats, provoking a significantly lower level of an allergic response in humans than normal cats. Their most significant feature is their coat, which is made up of soft waves, curls, and ringlets, resembling a shaggy perm.\n","The LaPerm emerged around the early 1980s as a spontaneous mutation of cats bred for pest control. The breed founders were Linda and Richard Koehl from The Dalles, Oregon, whose cat Speedy gave birth to a curly-coated kitten, named Curly, from whom all LaPerms descend. The Kohls allowed a free-breeding colony of curly-coated cats to develop over a period of ten years before making contact with members of the cat fancy and initiating a formal breeding program.\n","The breed was named after their curly coat which bears resemblance to a shaggy perm. The name follows the Chinookan tradition of adopting French words while incorporating the definite article to create a new word; for example, in Chinook Wawa, 'pipe' is lapeep and 'apple' is lapom, (la pipe and la pomme, respectively, in French).[1]\n","The LaPerm is in many ways a cat of moderation with no extremes and is still true to its original type.[clarification needed] It does however have an unusual coat. The breed standard[2] describes a muscular foreign-type body, which is medium in size with longish legs and neck. The head is a modified wedge with rounded contours and a muzzle which is slightly broad of the wedge. In profile, the straight nose leads into a break between the eyes up to a flattish forehead. LaPerms also have rather broad noses, flared ears, and medium-large almond-shaped eyes.\n","Like other rexes, all colors and patterns are acceptable,[3] although tabbies, reds and torties are quite common due to their origins. Also, the unusual colors from the early days of the breed have been selected for, so lilac, chocolate and colorpoints are popular. Newer varieties such as ticked tabbies, shades and darker points are also being bred.[citation needed]\n","The coat itself is described as having a textured feel. It is not silky, having a certain drag on the hand like mohair. It is usually soft, although the shorthairs will have more texture to their coats. The coat is loose and springy and stands away from the body with no thick undercoat. The coat varies according to the season and the maturity of the cat but is essentially wavy or curly with the longest and most defined curls in the ruff and on the neck. There is also longer curly fur inside the ears, tufts at the ear tips and \"ear muffs\", or longer, silky hair on the backs of the ears. The longhairs have a curly plumed tail while the shorthairs have tails rather like bottle brushes, and both have long curled whiskers. The coat sometimes falls into a natural parting along the back.\n","The first LaPerms were those belonging to breed founders Linda and Dick Koehl at their farm in Oregon. The other breeders who joined Linda to work on the breed's initial development in the USA included Solveig Pfleuger[4] (Manawyddan), who was a well-respected feline geneticist, Anne D Lawrence (Uluru), Beth Fillman (Calicorose) and Dee Borgardt (Deebor and Dairyland). Still, during the early days of the breeding program, they were joined by other breeders, including Pete Meisinger & Donna Lawry (Woodlandacre and Hattkatts), Maureen Neidhardt (Lakotaspirit), Lynne Daggett (Lowriders) and Mary Sharum (Sekani). The LaPerm Society of America (LPSA)[5] was formed in 1997 and became affiliated to CFA, helping to push the breed forward in that organization.\n","Valued members of the LPSA who have contributed to the breed's development and whose prefixes are seen in key LaPerm pedigrees include Erika Fetz (Vankkadia), Cheryl Cook (PacificGem) Diane Dunn (Lakme), Andrea Brew (Moonrise), Sandy Brew (Sunfall), Dennis Ganoe (Dennigan) and Debbie Estep (Shoalwater). When TICA finally approved championship status for the LaPerm in 2003 the all-important first cat to become a champion was Ch Dennigan's French Maid of Shoalwater, bred by Dennis Ganoe and owned by Debbie Estep. The breed gained championship recognition in CFA in May 2008 and the first champion was Ch Sunfall's BC Kahaha Towanjila. The first grand was Grand Premier Uluru BC Cloudfeet of CavalierCats owned by Cathy Hurley.[citation needed]\n","The first LaPerm in the UK was Champion Uluru BC Omaste Po of Quincunx, a lilac tortie and white Longhair who was bred in the United States by A. D. Lawrence and Maureen Neidhardt. She was imported by Anthony Nichols (Quincunx) using a PETS pet passport in May 2002 after a stop-over with LaPerm breeder Corine Judkins in the Netherlands. She arrived pregnantly and gave birth to a litter of five kittens shortly after who were used as the foundation stock for the UK breeding program. A number of other imports followed, including cats from Europe, New Zealand, and the USA. Judy Whiteford (Aswani) and Kate Munslow (Canonna) have been involved from that first litter and have both imported new cats themselves and Corine Judkins (Crearwy) moved to Wales bringing her cats with her including the stud who sired the first UK litter. Other key breeding lines found in UK pedigrees include those of Edwina Sipos (Cicada), Penni Cragg (Wakanda), June Gillies (Gallego), Kate Ekanger (Cloudborn), Sue Amor (Amorcatz) and Sue Pyrke (Bane). The breeding program has been characterized by efforts to breed down from outcrosses for generational advancement by combining outcross lines, old lines, and import lines.\n","The UK now has an active LaPerm breeding program and is the home of the LaPerm Cat Club. The breed has made solid progress within the GCCF and is often seen at British cat shows. In 2004 the breed gained Preliminary Recognition and the LaPerm Cat Club was formed. In June 2008, the LaPerm gained Provisional Recognition in the GCCF and the first cat to gain an Intermediate Certificate was Aswani Miranda Keys. In June 2012, the LaPerm gained full championship recognition with the GCCF and the first certificate winner was also Aswani Miranda Keys. The first LaPerm to become a GCCF champion was a female, Ballego Happy-Gladys, who went on to also become the first Grand Champion, and the first LaPerm to become a GCCF premier was Pr Wakanda Harriet Potter. The first male champion was Ch Quincunx Umberto Ecarl. The first LaPerm with an Imperial title was also Aswani Miranda Keys, the title being gained at the world's first LaPerm breed show, which was held by the LaPerm Cat Club.[6] The first male LaPerm with an imperial title was Imperial Grand Premier Cloudborn Barb Dwyer, bred by Kate Ekanger and owned by Nicola and Roy Lovell.[citation needed]\n","Breeding programs for LaPerms have spread to many other countries around the world. The breed was brought to Canada by Constance & Martine Sansoucy (Butterpaws), to New Zealand by Twink McCabe (Coiffurr) and Glynne Jackson (Wakijaki), to Australia by Christine Brelsford (Curlz) and later by Anne-Louise Magee (Frisson), to South Africa by Johan Lamprecht (Les Beaux Chats) and later by Grant Leih (Silkenclaw). LaPerms are also present in Japan, having first being exported there in 1997 by Anne D Lawrence. In continental Europe the first LaPerms were imported to Germany by Sabine Albrecht (Isanyati), these included the first LaPerm champion, Ch Uluru BC Wiyaka. However, it was Sylvie Groenveld (Smeralda's) who led the breeding program in that country. The initial imports to the Netherlands went to Corine Judkins (Crearwy) and a breed club was set up: the LaPerm Raskatten Vereniging, with key prefixes belonging to Frank and Rina Stapel (Taricats), Karin Langeveld (Takoda) and Angela Bruynswyck (Brunswick's).\n","The first Scandinavian breeder was Elinore Kopp (Shangri-La) in Sweden who imported Grand Champion Quincunx Qinkifurr and Champion Crearwy BC Madryn Merch Cari from the UK. The first Russian breeder was Svetlana Ponomareva (Russicurl). The first LaPerm in Taiwan was Triple Tiara Newron, bred by Yumi Masuda and imported from Japan by Archi Wang. Several other countries now also have LaPerms and the breed's popularity continues to spread. Provisional recognition was granted by FIFe in 2013, effective from 1 January 2014, which the first titled LaPerm in FIFe being Champion S*Bla Katten KombiSmart.[citation needed]\n","Breeding policies vary slightly between registries, but all encourage the occasional use of controlled outcrossing to maintain healthy genetic diversity within the breed's gene pool. A small range of pedigree breeds have been approved, as well as non-pedigree domestic cats. When undertaking outcross matings to non-pedigrees, reputable breeders seek out cats closely resembling the correct LaPerm body type with coats that are not overly thick. This practice continues the use of the kind of cats which composed much of the original foundation stock for the breed and helps to maintain genetic health by using the widest gene pool available. However, in some countries, such as the UK, there can be legal complications to selling kittens from such matings as pedigrees because of the Trades Description Act 1968[7] through which it has been established that the legal definition of a pedigree cat in the UK is normally one with a fully recorded three-generation pedigree. After outcrossing to a cat of unknown parentage, at least three generations must be bred to establish a full pedigree record.\n","In TICA outcrossing has mainly been with the domestic short-haired cat and domestic long-haired cat, although registration rules do allow other breeds to be used and bred down from towards the F3 generation which is eligible for entry in TICA cat shows.\n","In Cat Fanciers' Association (CFA) breeders used the Ocicat for a two-year period, terminating on 1 May 2002; LaPerms registered during this period were permitted to have an Ocicat parent, and by extension, one or two Abyssinian grandparents, as the Abyssinian is an approved outcross of the Ocicat. Currently, CFA breeders may only use non-pedigree domestic cats and after 2025[8] no outcrosses will be permitted in CFA. However, CFA accepts LaPerms for both breedings and showing with other breeds in their pedigrees if they are imported from another registry.\n","The GCCF has the most strict of the registration policies and only LaPerms with a full three-generation pedigree (i.e. parents, grandparents, and great-grandparents) of the only LaPerm to LaPerm breeding are permitted on the full register. Only LaPerms or cats from a list of approved breeds[9] are permitted in the 4th and 5th generations. Cats with non-approved breeds anywhere within their five-generation pedigrees, particularly those with other rex genes, cannot be registered as LaPerms. In order not to cause any damaging restriction to the breed's genepool a supplementary register also exists for the registration of LaPerms bred as part of an outcross breeding program. LaPerms can only be registered on the supplementary register if within their five-generation pedigrees only LaPerms and cats from the approved outcross list are present. In the GCCF this list comprises the Somali/Abyssinian, Asian/Tiffanie/(European)Burmese, Ocicat and Tonkinese. Domestic Shorthairs and Domestic Longhairs can be used in outcrossing but certain restrictions apply and the initial offspring are placed on the reference register and cannot be shown without first being assessed and approved by three judges. In other registries the approved list (with some slight variations) is used for outcrossing and cats of unknown parentage are not always permitted.\n","In FIFe, which has its most active LaPerm breeders in Sweden and the Netherlands, outcrossing is done on a case by case basis. In antipodean countries, Somalis, Tiffanies and Orientals have also been used, but Domestic Shorthairs and Domestic Longhairs are now the preferred choices of an outcross.\n","\n","\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"jxxqGNBamoPZ","executionInfo":{"status":"ok","timestamp":1720448171023,"user_tz":-180,"elapsed":2,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"outputs":[],"source":["def clean_text(text):\n","  # Remove non-alphanumeric characters\n","    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n","    text = text.lower()  # Lowercase\n","    return text\n","\n","def tokenize_and_lemmatize(text):\n","  stop_words = set(stopwords.words('english'))\n","  tokens = [word for word in word_tokenize(text) if word not in stop_words]\n","  lemmatizer = WordNetLemmatizer()\n","  return [lemmatizer.lemmatize(word) for word in tokens]\n"]},{"cell_type":"code","source":["cleaned_text = clean_text(text_data)\n","tokens = tokenize_and_lemmatize(cleaned_text)"],"metadata":{"id":"wJyWjAvx2ZbR","executionInfo":{"status":"ok","timestamp":1720448175917,"user_tz":-180,"elapsed":4895,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"bTR1TTiUqTDj","executionInfo":{"status":"ok","timestamp":1720448175918,"user_tz":-180,"elapsed":10,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([tokens])\n","sequences = tokenizer.texts_to_sequences([tokens])[0]  # Get sequences from single text\n","\n","# Hyperparameters (adjust as needed)\n","max_len = 50\n","vocab_size = len(tokenizer.word_index) + 1\n","embedding_dim = 128\n","\n","# Create Sequences and Labels\n","input_sequences = []\n","labels = []\n","\n","for i in range(0, len(sequences) - max_len):\n","    input_sequences.append(sequences[i:i + max_len])\n","    labels.append(sequences[i + max_len])\n","\n","# Convert to numpy arrays\n","input_sequences = np.array(input_sequences)\n","labels = np.array(labels)\n","\n","# Convert labels to one-hot encoding\n","labels = tf.keras.utils.to_categorical(labels, num_classes=vocab_size)\n","\n","# Pad sequences\n","input_sequences = pad_sequences(input_sequences, maxlen=max_len, padding='post')\n"],"metadata":{"id":"ivT_UgFCpOT4","executionInfo":{"status":"ok","timestamp":1720448175918,"user_tz":-180,"elapsed":10,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Model Building\n","def build_rnn_model():\n","    model = Sequential([\n","        Embedding(vocab_size, embedding_dim, input_length=max_len),\n","        SimpleRNN(units=128, return_sequences=False),\n","        Dense(vocab_size, activation='softmax')\n","    ])\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    return model\n","\n","def build_lstm_model():\n","    model = Sequential([\n","        Embedding(vocab_size, embedding_dim, input_length=max_len),\n","        LSTM(units=64, return_sequences=False),\n","        Dense(vocab_size, activation='softmax')\n","    ])\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    return model\n"],"metadata":{"id":"LHJ9x3PXpUfJ","executionInfo":{"status":"ok","timestamp":1720451829491,"user_tz":-180,"elapsed":2,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.optimizers import Adam"],"metadata":{"id":"Jn8BG0jd5wem","executionInfo":{"status":"ok","timestamp":1720448175918,"user_tz":-180,"elapsed":10,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","train_sequences, val_sequences, train_labels, val_labels = train_test_split(input_sequences, labels, test_size=0.2)"],"metadata":{"id":"fr9sfUS4wjED","executionInfo":{"status":"ok","timestamp":1720450255713,"user_tz":-180,"elapsed":336,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["lstm_model = build_lstm_model()\n","lstm_model.fit(input_sequences, labels, epochs=60, batch_size=32, validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"piPPqfA87Vww","executionInfo":{"status":"ok","timestamp":1720451594505,"user_tz":-180,"elapsed":146940,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"7d97c4e1-9f16-44a6-af22-ed1b9429623b","collapsed":true},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/60\n","29/29 [==============================] - 8s 150ms/step - loss: 6.5076 - accuracy: 0.0196 - val_loss: 6.5088 - val_accuracy: 0.0130\n","Epoch 2/60\n","29/29 [==============================] - 3s 96ms/step - loss: 6.3885 - accuracy: 0.0261 - val_loss: 6.8769 - val_accuracy: 0.0130\n","Epoch 3/60\n","29/29 [==============================] - 2s 82ms/step - loss: 6.1622 - accuracy: 0.0250 - val_loss: 7.1295 - val_accuracy: 0.0130\n","Epoch 4/60\n","29/29 [==============================] - 3s 88ms/step - loss: 6.0759 - accuracy: 0.0250 - val_loss: 7.3615 - val_accuracy: 0.0130\n","Epoch 5/60\n","29/29 [==============================] - 1s 48ms/step - loss: 6.0137 - accuracy: 0.0294 - val_loss: 7.4833 - val_accuracy: 0.0217\n","Epoch 6/60\n","29/29 [==============================] - 1s 45ms/step - loss: 5.9065 - accuracy: 0.0294 - val_loss: 7.3890 - val_accuracy: 0.0435\n","Epoch 7/60\n","29/29 [==============================] - 1s 44ms/step - loss: 5.7900 - accuracy: 0.0381 - val_loss: 7.6352 - val_accuracy: 0.0391\n","Epoch 8/60\n","29/29 [==============================] - 2s 70ms/step - loss: 5.6548 - accuracy: 0.0403 - val_loss: 7.6494 - val_accuracy: 0.0435\n","Epoch 9/60\n","29/29 [==============================] - 2s 72ms/step - loss: 5.5156 - accuracy: 0.0424 - val_loss: 7.6921 - val_accuracy: 0.0435\n","Epoch 10/60\n","29/29 [==============================] - 1s 44ms/step - loss: 5.3672 - accuracy: 0.0468 - val_loss: 7.8724 - val_accuracy: 0.0304\n","Epoch 11/60\n","29/29 [==============================] - 1s 45ms/step - loss: 5.2142 - accuracy: 0.0511 - val_loss: 8.0288 - val_accuracy: 0.0348\n","Epoch 12/60\n","29/29 [==============================] - 1s 44ms/step - loss: 5.0639 - accuracy: 0.0620 - val_loss: 8.1473 - val_accuracy: 0.0261\n","Epoch 13/60\n","29/29 [==============================] - 1s 44ms/step - loss: 4.9124 - accuracy: 0.0664 - val_loss: 8.3444 - val_accuracy: 0.0087\n","Epoch 14/60\n","29/29 [==============================] - 1s 45ms/step - loss: 4.7592 - accuracy: 0.0860 - val_loss: 8.3241 - val_accuracy: 0.0217\n","Epoch 15/60\n","29/29 [==============================] - 1s 43ms/step - loss: 4.6021 - accuracy: 0.0958 - val_loss: 8.4427 - val_accuracy: 0.0043\n","Epoch 16/60\n","29/29 [==============================] - 1s 44ms/step - loss: 4.4548 - accuracy: 0.0968 - val_loss: 8.3972 - val_accuracy: 0.0130\n","Epoch 17/60\n","29/29 [==============================] - 2s 54ms/step - loss: 4.3027 - accuracy: 0.1230 - val_loss: 8.4906 - val_accuracy: 0.0087\n","Epoch 18/60\n","29/29 [==============================] - 2s 73ms/step - loss: 4.1600 - accuracy: 0.1328 - val_loss: 8.6837 - val_accuracy: 0.0217\n","Epoch 19/60\n","29/29 [==============================] - 2s 59ms/step - loss: 4.0157 - accuracy: 0.1447 - val_loss: 8.6530 - val_accuracy: 0.0261\n","Epoch 20/60\n","29/29 [==============================] - 1s 46ms/step - loss: 3.8805 - accuracy: 0.1697 - val_loss: 8.7261 - val_accuracy: 0.0130\n","Epoch 21/60\n","29/29 [==============================] - 1s 44ms/step - loss: 3.7514 - accuracy: 0.1774 - val_loss: 8.7608 - val_accuracy: 0.0217\n","Epoch 22/60\n","29/29 [==============================] - 1s 45ms/step - loss: 3.6309 - accuracy: 0.2013 - val_loss: 8.7707 - val_accuracy: 0.0217\n","Epoch 23/60\n","29/29 [==============================] - 1s 45ms/step - loss: 3.5106 - accuracy: 0.2318 - val_loss: 8.8150 - val_accuracy: 0.0217\n","Epoch 24/60\n","29/29 [==============================] - 1s 44ms/step - loss: 3.4000 - accuracy: 0.2459 - val_loss: 8.9413 - val_accuracy: 0.0130\n","Epoch 25/60\n","29/29 [==============================] - 1s 43ms/step - loss: 3.2871 - accuracy: 0.2699 - val_loss: 8.9166 - val_accuracy: 0.0261\n","Epoch 26/60\n","29/29 [==============================] - 1s 43ms/step - loss: 3.1869 - accuracy: 0.2938 - val_loss: 8.9967 - val_accuracy: 0.0130\n","Epoch 27/60\n","29/29 [==============================] - 2s 66ms/step - loss: 3.0871 - accuracy: 0.3188 - val_loss: 9.0063 - val_accuracy: 0.0174\n","Epoch 28/60\n","29/29 [==============================] - 2s 73ms/step - loss: 2.9926 - accuracy: 0.3308 - val_loss: 9.1439 - val_accuracy: 0.0130\n","Epoch 29/60\n","29/29 [==============================] - 1s 51ms/step - loss: 2.9007 - accuracy: 0.3700 - val_loss: 9.0609 - val_accuracy: 0.0174\n","Epoch 30/60\n","29/29 [==============================] - 1s 45ms/step - loss: 2.8130 - accuracy: 0.3787 - val_loss: 9.0503 - val_accuracy: 0.0130\n","Epoch 31/60\n","29/29 [==============================] - 1s 46ms/step - loss: 2.7430 - accuracy: 0.4157 - val_loss: 9.0961 - val_accuracy: 0.0174\n","Epoch 32/60\n","29/29 [==============================] - 1s 44ms/step - loss: 2.6544 - accuracy: 0.4440 - val_loss: 9.2518 - val_accuracy: 0.0130\n","Epoch 33/60\n","29/29 [==============================] - 1s 44ms/step - loss: 2.5741 - accuracy: 0.4690 - val_loss: 9.2068 - val_accuracy: 0.0087\n","Epoch 34/60\n","29/29 [==============================] - 1s 46ms/step - loss: 2.4973 - accuracy: 0.5038 - val_loss: 9.2482 - val_accuracy: 0.0174\n","Epoch 35/60\n","29/29 [==============================] - 1s 45ms/step - loss: 2.4275 - accuracy: 0.5299 - val_loss: 9.2555 - val_accuracy: 0.0174\n","Epoch 36/60\n","29/29 [==============================] - 1s 45ms/step - loss: 2.3541 - accuracy: 0.5615 - val_loss: 9.2641 - val_accuracy: 0.0217\n","Epoch 37/60\n","29/29 [==============================] - 2s 72ms/step - loss: 2.2922 - accuracy: 0.5778 - val_loss: 9.2946 - val_accuracy: 0.0174\n","Epoch 38/60\n","29/29 [==============================] - 2s 67ms/step - loss: 2.2279 - accuracy: 0.6104 - val_loss: 9.3146 - val_accuracy: 0.0130\n","Epoch 39/60\n","29/29 [==============================] - 1s 44ms/step - loss: 2.1645 - accuracy: 0.6224 - val_loss: 9.3566 - val_accuracy: 0.0130\n","Epoch 40/60\n","29/29 [==============================] - 1s 47ms/step - loss: 2.1042 - accuracy: 0.6496 - val_loss: 9.3550 - val_accuracy: 0.0217\n","Epoch 41/60\n","29/29 [==============================] - 1s 48ms/step - loss: 2.0424 - accuracy: 0.6670 - val_loss: 9.3446 - val_accuracy: 0.0130\n","Epoch 42/60\n","29/29 [==============================] - 1s 44ms/step - loss: 1.9852 - accuracy: 0.7008 - val_loss: 9.4777 - val_accuracy: 0.0174\n","Epoch 43/60\n","29/29 [==============================] - 1s 45ms/step - loss: 1.9330 - accuracy: 0.7062 - val_loss: 9.4173 - val_accuracy: 0.0130\n","Epoch 44/60\n","29/29 [==============================] - 1s 45ms/step - loss: 1.8849 - accuracy: 0.7323 - val_loss: 9.3969 - val_accuracy: 0.0174\n","Epoch 45/60\n","29/29 [==============================] - 1s 44ms/step - loss: 1.8302 - accuracy: 0.7367 - val_loss: 9.4195 - val_accuracy: 0.0130\n","Epoch 46/60\n","29/29 [==============================] - 2s 63ms/step - loss: 1.7783 - accuracy: 0.7454 - val_loss: 9.4742 - val_accuracy: 0.0174\n","Epoch 47/60\n","29/29 [==============================] - 2s 82ms/step - loss: 1.7290 - accuracy: 0.7661 - val_loss: 9.4354 - val_accuracy: 0.0174\n","Epoch 48/60\n","29/29 [==============================] - 1s 44ms/step - loss: 1.6805 - accuracy: 0.7889 - val_loss: 9.4818 - val_accuracy: 0.0130\n","Epoch 49/60\n","29/29 [==============================] - 1s 46ms/step - loss: 1.6399 - accuracy: 0.7933 - val_loss: 9.5039 - val_accuracy: 0.0130\n","Epoch 50/60\n","29/29 [==============================] - 1s 45ms/step - loss: 1.5950 - accuracy: 0.8139 - val_loss: 9.5464 - val_accuracy: 0.0130\n","Epoch 51/60\n","29/29 [==============================] - 1s 45ms/step - loss: 1.5498 - accuracy: 0.8248 - val_loss: 9.5172 - val_accuracy: 0.0087\n","Epoch 52/60\n","29/29 [==============================] - 1s 46ms/step - loss: 1.5061 - accuracy: 0.8487 - val_loss: 9.4811 - val_accuracy: 0.0087\n","Epoch 53/60\n","29/29 [==============================] - 1s 45ms/step - loss: 1.4623 - accuracy: 0.8466 - val_loss: 9.5032 - val_accuracy: 0.0130\n","Epoch 54/60\n","29/29 [==============================] - 1s 44ms/step - loss: 1.4228 - accuracy: 0.8575 - val_loss: 9.5011 - val_accuracy: 0.0130\n","Epoch 55/60\n","29/29 [==============================] - 2s 62ms/step - loss: 1.3866 - accuracy: 0.8662 - val_loss: 9.5751 - val_accuracy: 0.0087\n","Epoch 56/60\n","29/29 [==============================] - 2s 75ms/step - loss: 1.3501 - accuracy: 0.8662 - val_loss: 9.4773 - val_accuracy: 0.0000e+00\n","Epoch 57/60\n","29/29 [==============================] - 1s 47ms/step - loss: 1.3144 - accuracy: 0.8770 - val_loss: 9.5476 - val_accuracy: 0.0087\n","Epoch 58/60\n","29/29 [==============================] - 1s 46ms/step - loss: 1.2775 - accuracy: 0.8879 - val_loss: 9.4891 - val_accuracy: 0.0043\n","Epoch 59/60\n","29/29 [==============================] - 1s 46ms/step - loss: 1.2375 - accuracy: 0.9064 - val_loss: 9.6061 - val_accuracy: 0.0043\n","Epoch 60/60\n","29/29 [==============================] - 2s 85ms/step - loss: 1.2018 - accuracy: 0.9119 - val_loss: 9.6360 - val_accuracy: 0.0130\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7a54560ddc00>"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["rnn_model = build_rnn_model()\n","rnn_model.fit(input_sequences, labels, epochs=25, batch_size=32, validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wQQB53XFpbew","executionInfo":{"status":"ok","timestamp":1720451880591,"user_tz":-180,"elapsed":45631,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"06a2e0fb-2d8f-4fb8-dc0c-13773d12b759","collapsed":true},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n","29/29 [==============================] - 6s 72ms/step - loss: 6.5159 - accuracy: 0.0076 - val_loss: 6.5440 - val_accuracy: 0.0478\n","Epoch 2/25\n","29/29 [==============================] - 1s 51ms/step - loss: 6.2483 - accuracy: 0.0250 - val_loss: 7.0104 - val_accuracy: 0.0478\n","Epoch 3/25\n","29/29 [==============================] - 1s 43ms/step - loss: 6.0654 - accuracy: 0.0294 - val_loss: 6.7108 - val_accuracy: 0.0391\n","Epoch 4/25\n","29/29 [==============================] - 1s 30ms/step - loss: 5.9063 - accuracy: 0.0555 - val_loss: 7.1023 - val_accuracy: 0.0130\n","Epoch 5/25\n","29/29 [==============================] - 1s 34ms/step - loss: 5.6862 - accuracy: 0.0566 - val_loss: 7.1864 - val_accuracy: 0.0087\n","Epoch 6/25\n","29/29 [==============================] - 1s 29ms/step - loss: 5.3951 - accuracy: 0.0816 - val_loss: 7.2808 - val_accuracy: 0.0130\n","Epoch 7/25\n","29/29 [==============================] - 1s 29ms/step - loss: 5.0516 - accuracy: 0.1458 - val_loss: 7.2069 - val_accuracy: 0.0043\n","Epoch 8/25\n","29/29 [==============================] - 1s 32ms/step - loss: 4.6687 - accuracy: 0.2242 - val_loss: 7.3860 - val_accuracy: 0.0130\n","Epoch 9/25\n","29/29 [==============================] - 1s 31ms/step - loss: 4.2777 - accuracy: 0.3428 - val_loss: 7.5003 - val_accuracy: 0.0043\n","Epoch 10/25\n","29/29 [==============================] - 1s 48ms/step - loss: 3.8658 - accuracy: 0.4548 - val_loss: 7.4165 - val_accuracy: 0.0087\n","Epoch 11/25\n","29/29 [==============================] - 1s 48ms/step - loss: 3.4452 - accuracy: 0.5919 - val_loss: 7.5190 - val_accuracy: 0.0043\n","Epoch 12/25\n","29/29 [==============================] - 1s 46ms/step - loss: 3.0349 - accuracy: 0.6812 - val_loss: 7.5665 - val_accuracy: 0.0043\n","Epoch 13/25\n","29/29 [==============================] - 1s 29ms/step - loss: 2.6242 - accuracy: 0.7726 - val_loss: 7.6443 - val_accuracy: 0.0000e+00\n","Epoch 14/25\n","29/29 [==============================] - 1s 29ms/step - loss: 2.2605 - accuracy: 0.8433 - val_loss: 7.7950 - val_accuracy: 0.0000e+00\n","Epoch 15/25\n","29/29 [==============================] - 1s 29ms/step - loss: 1.9111 - accuracy: 0.8966 - val_loss: 7.8047 - val_accuracy: 0.0130\n","Epoch 16/25\n","29/29 [==============================] - 1s 30ms/step - loss: 1.6006 - accuracy: 0.9347 - val_loss: 7.7671 - val_accuracy: 0.0043\n","Epoch 17/25\n","29/29 [==============================] - 1s 31ms/step - loss: 1.3156 - accuracy: 0.9576 - val_loss: 7.8784 - val_accuracy: 0.0087\n","Epoch 18/25\n","29/29 [==============================] - 1s 29ms/step - loss: 1.0718 - accuracy: 0.9815 - val_loss: 7.8720 - val_accuracy: 0.0087\n","Epoch 19/25\n","29/29 [==============================] - 1s 29ms/step - loss: 0.8721 - accuracy: 0.9913 - val_loss: 8.0798 - val_accuracy: 0.0043\n","Epoch 20/25\n","29/29 [==============================] - 1s 30ms/step - loss: 0.7170 - accuracy: 0.9924 - val_loss: 8.0992 - val_accuracy: 0.0000e+00\n","Epoch 21/25\n","29/29 [==============================] - 1s 30ms/step - loss: 0.5842 - accuracy: 1.0000 - val_loss: 8.1665 - val_accuracy: 0.0087\n","Epoch 22/25\n","29/29 [==============================] - 1s 30ms/step - loss: 0.4878 - accuracy: 1.0000 - val_loss: 8.1876 - val_accuracy: 0.0043\n","Epoch 23/25\n","29/29 [==============================] - 1s 33ms/step - loss: 0.4036 - accuracy: 1.0000 - val_loss: 8.2158 - val_accuracy: 0.0043\n","Epoch 24/25\n","29/29 [==============================] - 1s 43ms/step - loss: 0.3335 - accuracy: 1.0000 - val_loss: 8.2230 - val_accuracy: 0.0043\n","Epoch 25/25\n","29/29 [==============================] - 1s 49ms/step - loss: 0.2770 - accuracy: 1.0000 - val_loss: 8.3152 - val_accuracy: 0.0043\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7a5457045d20>"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["def predict_next_word(model, sequence):\n","  padded_sequence = pad_sequences([sequence], maxlen=max_len, padding='post')\n","  predictions = model.predict(padded_sequence)[0]\n","  return tokenizer.index_word[tf.math.argmax(predictions).numpy()]\n"],"metadata":{"id":"Rbk1yh4Mpo-8","executionInfo":{"status":"ok","timestamp":1720448373167,"user_tz":-180,"elapsed":10,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["seed_text = \"Their most significant feature is their coat, which is made up of soft waves, curls, and ringlets, resembling a shaggy\"\n","cleaned_text = clean_text(seed_text)\n","tokens = tokenize_and_lemmatize(cleaned_text)\n","seed_sequence = tokenizer.texts_to_sequences([tokens])[0]\n","\n","predicted_word_rnn = predict_next_word(rnn_model, seed_sequence)\n","predicted_word_lstm = predict_next_word(lstm_model, seed_sequence)\n","\n","print(f\"RNN predicted next word: {predicted_word_rnn}\")\n","print(f\"LSTM predicted next word: {predicted_word_lstm}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NA5z89h3t-ha","executionInfo":{"status":"ok","timestamp":1720451883734,"user_tz":-180,"elapsed":734,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"fd899085-90a1-47f1-df53-165e781580c9"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 273ms/step\n","1/1 [==============================] - 0s 38ms/step\n","RNN predicted next word: breeding\n","LSTM predicted next word: 1997\n"]}]},{"cell_type":"code","source":["seed_text = \"The breed standard describes a muscular foreign-type body, which is medium in size with longish legs and\"\n","cleaned_text = clean_text(seed_text)\n","tokens = tokenize_and_lemmatize(cleaned_text)\n","seed_sequence = tokenizer.texts_to_sequences([tokens])[0]\n","\n","predicted_word_rnn = predict_next_word(rnn_model, seed_sequence)\n","predicted_word_lstm = predict_next_word(lstm_model, seed_sequence)\n","\n","print(f\"RNN predicted next word: {predicted_word_rnn}\")\n","print(f\"LSTM predicted next word: {predicted_word_lstm}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CA9ySTjL3pRM","executionInfo":{"status":"ok","timestamp":1720451887751,"user_tz":-180,"elapsed":498,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"62075abd-96b2-4b3b-91e1-284462348b3b"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","RNN predicted next word: bred\n","LSTM predicted next word: breed\n"]}]},{"cell_type":"code","source":["def calculate_perplexity(model, sequences, max_length):\n","    total_log_prob = 0.0\n","    total_words = 0\n","\n","    for seq in sequences:\n","        input_seq = seq[:-1]\n","        target_seq = seq[1:]\n","\n","        input_padded = pad_sequences([input_seq], maxlen=max_length, padding='post')\n","        preds = model.predict(input_padded, verbose=0)[0]\n","\n","        # Ensure preds is 1D array and handle it correctly\n","        for t, word in enumerate(target_seq):\n","            if word == 0:\n","                break\n","            total_log_prob += np.log(preds[word])  # preds[word] instead of preds[t, word]\n","            total_words += 1\n","\n","    perplexity = np.exp(-total_log_prob / total_words)\n","    return perplexity\n"],"metadata":{"id":"t9G-Whmssnw6","executionInfo":{"status":"ok","timestamp":1720449278077,"user_tz":-180,"elapsed":408,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["def evaluate_accuracy(model, sequences, max_length):\n","    correct_preds = 0\n","    total_preds = 0\n","\n","    for seq in sequences:\n","        input_seq = seq[:-1]\n","        target_seq = seq[1:]\n","\n","        input_padded = pad_sequences([input_seq], maxlen=max_length, padding='post')\n","        preds = model.predict(input_padded, verbose=0)[0]\n","\n","        for t, word in enumerate(target_seq):\n","            if word == 0:\n","                break\n","            predicted_word = np.argmax(preds)  # Get the predicted word index from 1D preds\n","            if predicted_word == word:\n","                correct_preds += 1\n","            total_preds += 1\n","\n","    accuracy = correct_preds / total_preds\n","    return accuracy\n"],"metadata":{"id":"LUMqfyygso0p","executionInfo":{"status":"ok","timestamp":1720449288789,"user_tz":-180,"elapsed":347,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["rnn_perplexity = calculate_perplexity(rnn_model, val_sequences, max_len)\n","rnn_accuracy = evaluate_accuracy(rnn_model, val_sequences, max_len)\n","\n","print(f\"RNN Model Perplexity: {rnn_perplexity}\")\n","print(f\"RNN Model Accuracy: {rnn_accuracy}\")\n","\n","\n","lstm_perplexity = calculate_perplexity(lstm_model, val_sequences, max_len)\n","lstm_accuracy = evaluate_accuracy(lstm_model, val_sequences, max_len)\n","\n","print(f\"LSTM Model Perplexity: {lstm_perplexity}\")\n","print(f\"LSTM Model Accuracy: {lstm_accuracy}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ce3eWfFqsvYk","executionInfo":{"status":"ok","timestamp":1720452002549,"user_tz":-180,"elapsed":106197,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"a7965afc-4d32-4eb0-b267-42a79a45b20e"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["RNN Model Perplexity: 4058.1804259379596\n","RNN Model Accuracy: 0.006388642413487134\n","LSTM Model Perplexity: 1715.1280243652964\n","LSTM Model Accuracy: 0.018278615794143745\n"]}]},{"cell_type":"markdown","source":["**Conclusion** **and comparison:**\n","\n","The RNN model has a higher perplexity (4058.18) compared to the LSTM model (1715.128), suggesting that the LSTM model is better at predicting the next word in the sequence.\n","The RNN model has a very low accuracy (0.64%) compared to the LSTM model (1.82%). While both accuracies are low, the LSTM model still outperforms the RNN model.\n","Some of the reasons for this are:\n","LSTMs have a more complex architecture with mechanisms (input, forget, and output gates) that help them retain important information over longer sequences.\n","Simple RNNs lack these mechanisms, making them less capable of handling dependencies in longer sequences.\n","\n","To receive better performance, we might tune the hyperparameters, add more layers. And, most importantly, use bigger and more diverse dataset for training."],"metadata":{"id":"t4VpZNWCyUEj"}},{"cell_type":"code","source":["!pip install transformers torch\n","!pip install scikit-learn\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vlgKmeNI0Rb0","executionInfo":{"status":"ok","timestamp":1720452153422,"user_tz":-180,"elapsed":111662,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"842f3d0e-4548-46c6-916e-898babbc4cf2"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"]}]},{"cell_type":"code","source":["def preprocess_text(text):\n","    sentences = nltk.sent_tokenize(text)\n","    return sentences\n","\n","def kl_sum(similarity_matrix, sentences, num_sentences=5):\n","    sentence_scores = np.sum(similarity_matrix, axis=1)\n","    ranked_sentences = [sentences[i] for i in np.argsort(sentence_scores)[-num_sentences:]]\n","    return ' '.join(ranked_sentences)\n","\n","sentences = preprocess_text(text_data)\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(sentences)\n","similarity_matrix = cosine_similarity(X, X)\n","summary = kl_sum(similarity_matrix, sentences, num_sentences=5)\n","\n","print(\"Summary:\")\n","print(summary)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KaSSnLEtvAbY","executionInfo":{"status":"ok","timestamp":1720452260342,"user_tz":-180,"elapsed":358,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"83744e7e-cb25-4375-cd34-987888541282"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Summary:\n","In June 2008, the LaPerm gained Provisional Recognition in the GCCF and the first cat to gain an Intermediate Certificate was Aswani Miranda Keys. The first LaPerm with an Imperial title was also Aswani Miranda Keys, the title being gained at the world's first LaPerm breed show, which was held by the LaPerm Cat Club. The coat varies according to the season and the maturity of the cat but is essentially wavy or curly with the longest and most defined curls in the ruff and on the neck. The UK now has an active LaPerm breeding program and is the home of the LaPerm Cat Club. The LaPerm is a breed of cat.\n"]}]},{"cell_type":"markdown","source":["# **Chat GPT model**"],"metadata":{"id":"uV4IwLNkeXN7"}},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"collapsed":true,"id":"-g7eUsAZHMkK","executionInfo":{"status":"ok","timestamp":1720452983223,"user_tz":-180,"elapsed":16863,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"e827108a-ab99-4313-b4e7-819480258cfa"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/547.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m542.7/547.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Collecting requests>=2.32.2 (from datasets)\n","  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pyarrow","requests"]},"id":"d6de1e4c992744beaa9627b3ca7b1872"}},"metadata":{}}]},{"cell_type":"code","source":["import torch\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments\n","from datasets import load_dataset, Dataset\n"],"metadata":{"id":"iyCuh9qaebLQ","executionInfo":{"status":"ok","timestamp":1720453245823,"user_tz":-180,"elapsed":19814,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!pip install transformers torch\n","from google.colab import files\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"PuduhC9ae6Uf","executionInfo":{"status":"ok","timestamp":1720453052380,"user_tz":-180,"elapsed":8663,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"5307a08c-25fa-4e6b-b5f0-ca010d155958"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}]},{"cell_type":"code","source":["with open(\"laperm_corpus.txt\", \"w\") as f:\n","    f.write(text_data)\n","\n","# Then download the file\n","#files.download(\"laperm_corpus.txt\")\n"],"metadata":{"id":"XI0FujWsfGBK","executionInfo":{"status":"ok","timestamp":1720453076543,"user_tz":-180,"elapsed":376,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!pip install transformers[torch]\n","!pip install accelerate -U\n","!pip install --upgrade accelerate"],"metadata":{"collapsed":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"g2o9ZjRtiEST","executionInfo":{"status":"ok","timestamp":1720453151691,"user_tz":-180,"elapsed":33411,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"bf45cfa5-8a73-4ea0-df59-ae99404e2f3d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n","Collecting accelerate>=0.21.0 (from transformers[torch])\n","  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.82)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.32.1\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n","Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"]}]},{"cell_type":"code","source":["# Load the corpus\n","dataset = load_dataset('text', data_files={'train': 'laperm_corpus.txt'})\n","\n","# Load pre-trained GPT-2 model and tokenizer\n","model_name = \"gpt2\"\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","\n","# Add a padding token to the tokenizer\n","tokenizer.pad_token = tokenizer.eos_token"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCgqAjvvdqtJ","executionInfo":{"status":"ok","timestamp":1720453254982,"user_tz":-180,"elapsed":4477,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"310ef868-329d-4a02-de11-dab621942b42"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Tokenize the dataset\n","def tokenize_function(examples):\n","    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=256)\n","\n","# Create labels by shifting inputs\n","def add_labels(examples):\n","    examples[\"labels\"] = examples[\"input_ids\"].copy()\n","    return examples\n"],"metadata":{"id":"maj8pJAodq8x","executionInfo":{"status":"ok","timestamp":1720453258736,"user_tz":-180,"elapsed":367,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n","\n","tokenized_datasets = tokenized_datasets.map(add_labels, batched=True)\n","tokenized_datasets.set_format(\"torch\")"],"metadata":{"id":"5ILF_M7DdrMq","executionInfo":{"status":"ok","timestamp":1720453262504,"user_tz":-180,"elapsed":2018,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Define the data collator\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False,  # Masked Language Modeling is not used here\n",")\n","\n","# Set training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./gpt2-laperm\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=3,  # Reduced number of epochs\n","    per_device_train_batch_size=2,  # Reduced batch size to lower memory usage\n","    save_steps=10_000,\n","    save_total_limit=2,\n","    fp16=True,  # Use mixed precision training\n","    gradient_accumulation_steps=2,  # Reduced gradient accumulation steps to reduce memory usage\n","    dataloader_num_workers=2,  # Reduced number of worker processes\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    data_collator=data_collator,\n",")\n","\n","# Fine-tune the model\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"id":"-3M26TNxd6NS","executionInfo":{"status":"ok","timestamp":1720453584615,"user_tz":-180,"elapsed":319622,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"dc577677-b0fa-423a-dedb-910a0c8bf676"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15/15 04:45, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=15, training_loss=3.977416229248047, metrics={'train_runtime': 317.841, 'train_samples_per_second': 0.179, 'train_steps_per_second': 0.047, 'total_flos': 7446822912000.0, 'train_loss': 3.977416229248047, 'epoch': 3.0})"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# Save the model and tokenizer\n","trainer.save_model(\"./gpt2-laperm\")\n","tokenizer.save_pretrained(\"./gpt2-laperm\")\n","\n","# Load the fine-tuned model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained(\"./gpt2-laperm\")\n","tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-laperm\")"],"metadata":{"id":"UminOGjreN81","executionInfo":{"status":"ok","timestamp":1720453606523,"user_tz":-180,"elapsed":8066,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Generate completions for partial sentences\n","partial_sentences = [\n","    \"The LaPerm is a rex breed which originated in the United States and is now\",\n","    \"They are reputed to be hypoallergenic cats, provoking a significantly lower level of\",\n","    \"Their most significant feature is their coat, which is made up of soft waves, curls,\",\n","    \"The LaPerm is in many ways a cat of moderation with no extremes and is still true to\",\n","    \"The coat varies according to the season and the maturity of the cat but is essentially wavy or \"\n","]\n","\n","for sentence in partial_sentences:\n","    inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n","    attention_mask = inputs != tokenizer.pad_token_id\n","    outputs = model.generate(\n","        inputs,\n","        attention_mask=attention_mask,\n","        max_length=40,\n","        num_return_sequences=1,\n","        no_repeat_ngram_size=2,\n","        temperature=0.7,\n","        top_p=0.9,\n","        repetition_penalty=1.2,\n","        pad_token_id=tokenizer.eos_token_id,\n","        num_beams=5,  # Adding beam search for better quality\n","        early_stopping=True  # Stop when an EOS token is generated\n","    )\n","    print(f\"Original: {sentence}\")\n","    print(f\"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d-8hoLf_d-_g","executionInfo":{"status":"ok","timestamp":1720453646236,"user_tz":-180,"elapsed":31833,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"50d8712f-7826-4ce1-f8e0-13caf89d573b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Original: The LaPerm is a rex breed which originated in the United States and is now\n","Generated: The LaPerm is a rex breed which originated in the United States and is now found in many countries around the world. The breed was first introduced to the UK in 1997 and has since spread\n","\n","Original: They are reputed to be hypoallergenic cats, provoking a significantly lower level of\n","Generated: They are reputed to be hypoallergenic cats, provoking a significantly lower level of allergic reactions than other breeds. However, there is no scientific evidence to support the use of these cats for\n","\n","Original: Their most significant feature is their coat, which is made up of soft waves, curls,\n","Generated: Their most significant feature is their coat, which is made up of soft waves, curls, and long, curly hair. They are also known for their ability to grow up to six feet tall.\n","\n","\n","Original: The LaPerm is in many ways a cat of moderation with no extremes and is still true to\n","Generated: The LaPerm is in many ways a cat of moderation with no extremes and is still true to its name. It is also known as the \"cat of the year\" because of its large size\n","\n","Original: The coat varies according to the season and the maturity of the cat but is essentially wavy or \n","Generated: The coat varies according to the season and the maturity of the cat but is essentially wavy or ursine. The coat also has a long neck and a short tail.\n","\n","Cats with\n","\n"]}]},{"cell_type":"markdown","source":["# **Sentiment Analysis**"],"metadata":{"id":"LXHAIGmceAYG"}},{"cell_type":"code","source":["import nltk\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","from collections import Counter\n","\n","# Ensure you have downloaded the VADER lexicon\n","nltk.download('vader_lexicon')\n","\n","# Initialize VADER sentiment analyzer\n","sid = SentimentIntensityAnalyzer()\n","\n","\n","# Analyze sentiment for each sentence\n","sentiment_labels = []\n","\n","for sentence in sentences:\n","    scores = sid.polarity_scores(sentence)\n","    if scores['compound'] >= 0.05:\n","        sentiment_labels.append('positive')\n","    elif scores['compound'] <= -0.05:\n","        sentiment_labels.append('negative')\n","    else:\n","        sentiment_labels.append('neutral')\n","\n","# Count the occurrences of each sentiment\n","sentiment_counts = Counter(sentiment_labels)\n","total_sentences = len(sentences)\n","\n","# Calculate percentage distribution\n","sentiment_distribution = {label: count / total_sentences * 100 for label, count in sentiment_counts.items()}\n","\n","print(\"Sentiment Distribution:\")\n","print(f\"Positive: {sentiment_distribution.get('positive', 0):.2f}%\")\n","print(f\"Negative: {sentiment_distribution.get('negative', 0):.2f}%\")\n","print(f\"Neutral: {sentiment_distribution.get('neutral', 0):.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJe0ufeDsM-j","executionInfo":{"status":"ok","timestamp":1720452884523,"user_tz":-180,"elapsed":372,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"429a2f6b-c966-474c-daa4-51f572a3304c"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentiment Distribution:\n","Positive: 52.87%\n","Negative: 8.05%\n","Neutral: 39.08%\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n","[nltk_data]   Package vader_lexicon is already up-to-date!\n"]}]},{"cell_type":"code","source":["for sentence in sentences[:10]:  # Check the first 10 sentences\n","    scores = sid.polarity_scores(sentence)\n","    print(f\"Sentence: {sentence}\")\n","    print(f\"Scores: {scores}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"myKDvVUi-Zes","executionInfo":{"status":"ok","timestamp":1720452900735,"user_tz":-180,"elapsed":2,"user":{"displayName":"Miriam Sidorov","userId":"12445447305755891322"}},"outputId":"563bdba5-06da-4af5-d47f-a6a9791956e6"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence: The LaPerm is a breed of cat.\n","Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n","\n","Sentence: A LaPerm's fur is curly (hence the name \"perm\"), with the tightest curls being on the throat and on the base of the ears.\n","Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n","\n","Sentence: LaPerms come in many colors and patterns.\n","Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n","\n","Sentence: LaPerms generally have a very affectionate personality.\n","Scores: {'neg': 0.0, 'neu': 0.61, 'pos': 0.39, 'compound': 0.4927}\n","\n","Sentence: The LaPerm is a rex breed which originated in the United States and is now present in many other countries worldwide.\n","Scores: {'neg': 0.0, 'neu': 0.872, 'pos': 0.128, 'compound': 0.4215}\n","\n","Sentence: The breed is genetically unique and not related to any other rex cat varieties, having a dominant gene causing their curly coats.\n","Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n","\n","Sentence: They have an elegant and athletic build and are affectionate, active, and outgoing in character.\n","Scores: {'neg': 0.0, 'neu': 0.502, 'pos': 0.498, 'compound': 0.872}\n","\n","Sentence: They are reputed to be hypoallergenic cats, provoking a significantly lower level of an allergic response in humans than normal cats.\n","Scores: {'neg': 0.267, 'neu': 0.733, 'pos': 0.0, 'compound': -0.6369}\n","\n","Sentence: Their most significant feature is their coat, which is made up of soft waves, curls, and ringlets, resembling a shaggy perm.\n","Scores: {'neg': 0.0, 'neu': 0.901, 'pos': 0.099, 'compound': 0.2716}\n","\n","Sentence: The LaPerm emerged around the early 1980s as a spontaneous mutation of cats bred for pest control.\n","Scores: {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n","\n"]}]}]}