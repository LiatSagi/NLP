{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "56c3d448-2915-4f73-9e6d-bc9b6ff7189b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: tabulate in c:\\users\\user\\anaconda3\\lib\\site-packages (0.8.10)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install beautifulsoup4\n",
    "!pip install tabulate\n",
    "\n",
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4b604b7b-335c-4259-841d-e7d62eaa7b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import string\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tabulate import tabulate\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc75814e-2b95-420f-8a32-a7598a9e22fb",
   "metadata": {},
   "source": [
    "# 1. Use the corpus you created in homework 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "52177a6d-de34-4480-bd0a-814e16d0ba0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Selkirk Rex is a breed of cat with highly curled hair.[1]\n",
      "The Selkirk Rex is distinct from all other Rex breeds. Unlike the Devon Rex and Cornish Rex, the hair is of normal length and not partly missing. There are longhair and shorthair varieties.[2] It differs from the LaPerm in that the Selkirk Rex coat is plusher and thicker. While the LaPerm gene is a simple dominant, the Selkirk gene (Se) acts as an incomplete dominant; incompletely dominant allele pairs produce three possible genotypes and phenotypes: heterozygous cats (Sese) may have a fuller coat that is preferred in the show ring, while homozygous cats (SeSe) may have a tighter curl and less coat volume.\n",
      "The Selkirk Rex originated in Montana, USA in 1987, with a litter born to a rescued cat.[3] The only unusually coated kitten in the litter was ultimately placed with breeder, Jeri Newman, who named her Miss DiPesto (after a curly-haired character in the TV series Moonlighting played by Allyce Beasley). This foundation cat was bred to a black Persian tomcat, producing three Selkirk Rex and three straight-haired kittens. This demonstrated that the gene had an autosomal dominant mode of inheritance. All Selkirk Rex trace their ancestry back to the cat Miss DiPesto. Jeri Newman named the breed after her stepfather, \"Selkirk,\" making this the first (and currently only) breed of cat to be named after an actual person.[4]\n",
      "The breed has been developed in two coat lengths, long and short. It is a large and solidly built breed, similar to a British Shorthair. The coat is very soft and has a woolly look and feel with loose, unstructured curls. The head is round, with large rounded eyes, medium-sized ears, and a distinct muzzle, whose length is equal to half its width. An extreme break, like that of a Persian, is a disqualifiable fault.\n",
      "American Shorthairs, Persians, Himalayans, Exotic Shorthairs, and British Shorthairs have been used as outcrosses to develop this breed.[5] The American Shorthair has now been discontinued as an outcross, except in The International Cat Association (TICA). The breed was accepted by The International Cat Association in 1992,[6] the American Cat Fanciers Association in 1998, and the Cat Fanciers' Association in 2000. In Cat Fanciers' Association (CFA) and in Australia, all outcrosses are scheduled to be discontinued in 2015.\n",
      "The breed is accepted in all colors, including the pointed, sepia, and mink varieties of albinism; bicolors; cinnamon; silver/smoke; and the chocolate and lilac series. This breed has an extremely dense coat and high propensity for shedding. Unlike other Rex breeds with reduced amounts of hair, the Selkirk Rex is not recommended for those who might be allergic to cat allergens.\n",
      "The temperament of the Selkirk Rex reflects that of the breeds used in its development. They have a lot of the laid-back, reserved qualities of the British Shorthair, the cuddly nature of the Persian, and the playfulness of the Exotic Shorthair.\n",
      "There are no known health problems specific to the Selkirk Rex breed. They are a robust breed. Breeding towards proper head structure is necessary to prevent kinking of the tear ducts, resulting in tear run down the front of the face, or muzzle creases that can result in dermatitis on the face. Like other Rex breeds, irritation of the inside of the ear by curly fur can occur, increasing the production of ear wax. Homozygous cats (with two copies of the dominant Selkirk Rex gene) may have a tendency towards excessive greasiness of the coat, requiring increased frequency of bathing. Other health problems may be inherited from the outcross breeds used, including polycystic kidney disease from Persians and hypertrophic cardiomyopathy from British Shorthairs. Responsible breeders screen their breeding cats for these diseases to minimize their impact on the breed.\n",
      "In the UK, all Selkirk Rex registered with the Governing Council of the Cat Fancy (GCCF) for breeding are genetically tested for Polycystic Kidney Disease or are from two genetically tested parents.[7]\n",
      "The Selkirk Rex is defined by an autosomal dominant woolly rexoid hair (ADWH) abnormality that is characterized by tightly curled hair shafts.[8] A splice variant in the gene KRT71 was found to be associated with the curly coat phenotype. KRT71 is a crucial gene for keratinization of the hair follicle. An allele of this gene is also responsible for the hairless (hr) Sphynx and the Devon Rex (re) hair.[9] Three mutations in KRT71 have now been identified in cats, forming the allelic series, KRT71SADRE > KRT71+ > KRT71re > KRT71hr,[10] where SADRE is the suggested locus designation for the Selkirk autosomal dominant rex\n",
      "21 allele.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the URL of the Wikipedia page for Selkirk Rex\n",
    "url = \"https://en.wikipedia.org/wiki/Selkirk_Rex\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the container element that holds the main content of the page\n",
    "content_div = soup.find('div', {'class': 'mw-parser-output'})\n",
    "\n",
    "# Find all <p> tags within the content area\n",
    "if content_div:\n",
    "    paragraphs = content_div.find_all('p')\n",
    "\n",
    "    # Extract text from each paragraph and concatenate them into a single string\n",
    "    text_data = ''\n",
    "    for paragraph in paragraphs:\n",
    "        text_data += paragraph.get_text()\n",
    "\n",
    "    # Print the scraped text data\n",
    "    print(text_data)\n",
    "    all_words = text_data.split()\n",
    "else:\n",
    "    print(\"Main content container not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb8ccc-781b-4588-b966-920b8e19720e",
   "metadata": {},
   "source": [
    " # 2. tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5bffe8-4a52-4587-86ea-c22e5080c809",
   "metadata": {},
   "source": [
    "**White Space Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4c49166a-1296-4df4-a4b2-cfcf0d965658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Selkirk', 'Rex', 'is', 'a', 'breed', 'of', 'cat', 'with', 'highly', 'curled', 'hair.[1]', 'The', 'Selkirk', 'Rex', 'is', 'distinct', 'from', 'all', 'other', 'Rex', 'breeds.', 'Unlike', 'the', 'Devon', 'Rex', 'and', 'Cornish', 'Rex,', 'the', 'hair', 'is', 'of', 'normal', 'length', 'and', 'not', 'partly', 'missing.', 'There', 'are', 'longhair', 'and', 'shorthair', 'varieties.[2]', 'It', 'differs', 'from', 'the', 'LaPerm', 'in', 'that', 'the', 'Selkirk', 'Rex', 'coat', 'is', 'plusher', 'and', 'thicker.', 'While', 'the', 'LaPerm', 'gene', 'is', 'a', 'simple', 'dominant,', 'the', 'Selkirk', 'gene', '(Se)', 'acts', 'as', 'an', 'incomplete', 'dominant;', 'incompletely', 'dominant', 'allele', 'pairs', 'produce', 'three', 'possible', 'genotypes', 'and', 'phenotypes:', 'heterozygous', 'cats', '(Sese)', 'may', 'have', 'a', 'fuller', 'coat', 'that', 'is', 'preferred', 'in', 'the']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def whitespace_tokenize(text):\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Example usage:\n",
    "tokens_whitespace = whitespace_tokenize(text_data)\n",
    "print(tokens_whitespace[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014d8cd0-151e-4481-b08c-69c6ed847f81",
   "metadata": {},
   "source": [
    "**Word Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2c99523c-7119-4e24-8903-2e06891bdc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Selkirk', 'Rex', 'is', 'a', 'breed', 'of', 'cat', 'with', 'highly', 'curled', 'hair', '1', 'The', 'Selkirk', 'Rex', 'is', 'distinct', 'from', 'all', 'other', 'Rex', 'breeds', 'Unlike', 'the', 'Devon', 'Rex', 'and', 'Cornish', 'Rex', 'the', 'hair', 'is', 'of', 'normal', 'length', 'and', 'not', 'partly', 'missing', 'There', 'are', 'longhair', 'and', 'shorthair', 'varieties', '2', 'It', 'differs', 'from', 'the', 'LaPerm', 'in', 'that', 'the', 'Selkirk', 'Rex', 'coat', 'is', 'plusher', 'and', 'thicker', 'While', 'the', 'LaPerm', 'gene', 'is', 'a', 'simple', 'dominant', 'the', 'Selkirk', 'gene', 'Se', 'acts', 'as', 'an', 'incomplete', 'dominant', 'incompletely', 'dominant', 'allele', 'pairs', 'produce', 'three', 'possible', 'genotypes', 'and', 'phenotypes', 'heterozygous', 'cats', 'Sese', 'may', 'have', 'a', 'fuller', 'coat', 'that', 'is', 'preferred']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize_with_nltk(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    return tokens\n",
    "\n",
    "# Example usage:\n",
    "tokens_nltk = tokenize_with_nltk(text_data)\n",
    "print(tokens_nltk[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7d446-d134-49b0-adcd-b824c75f85ce",
   "metadata": {},
   "source": [
    " **Regex Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "96cf0d14-b72b-420f-b8a0-def26020d940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Selkirk', 'Rex', 'is', 'a', 'breed', 'of', 'cat', 'with', 'highly', 'curled', 'hair', '1', 'The', 'Selkirk', 'Rex', 'is', 'distinct', 'from', 'all', 'other', 'Rex', 'breeds', 'Unlike', 'the', 'Devon', 'Rex', 'and', 'Cornish', 'Rex', 'the', 'hair', 'is', 'of', 'normal', 'length', 'and', 'not', 'partly', 'missing', 'There', 'are', 'longhair', 'and', 'shorthair', 'varieties', '2', 'It', 'differs', 'from', 'the', 'LaPerm', 'in', 'that', 'the', 'Selkirk', 'Rex', 'coat', 'is', 'plusher', 'and', 'thicker', 'While', 'the', 'LaPerm', 'gene', 'is', 'a', 'simple', 'dominant', 'the', 'Selkirk', 'gene', 'Se', 'acts', 'as', 'an', 'incomplete', 'dominant', 'incompletely', 'dominant', 'allele', 'pairs', 'produce', 'three', 'possible', 'genotypes', 'and', 'phenotypes', 'heterozygous', 'cats', 'Sese', 'may', 'have', 'a', 'fuller', 'coat', 'that', 'is', 'preferred']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def regex_tokenize(text, pattern=r'\\w+'):\n",
    "    tokenizer = RegexpTokenizer(pattern)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Example usage:\n",
    "tokens_regex = regex_tokenize(text_data)\n",
    "print(tokens_regex[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf450c22-be03-4df4-9927-9db86f0c32b1",
   "metadata": {},
   "source": [
    "**Sentence Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d62d621b-7897-4db0-98e2-d6802d7cf082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Selkirk Rex is a breed of cat with highly curled hair.', '[1]\\nThe Selkirk Rex is distinct from all other Rex breeds.', 'Unlike the Devon Rex and Cornish Rex, the hair is of normal length and not partly missing.', 'There are longhair and shorthair varieties.', '[2] It differs from the LaPerm in that the Selkirk Rex coat is plusher and thicker.', 'While the LaPerm gene is a simple dominant, the Selkirk gene (Se) acts as an incomplete dominant; incompletely dominant allele pairs produce three possible genotypes and phenotypes: heterozygous cats (Sese) may have a fuller coat that is preferred in the show ring, while homozygous cats (SeSe) may have a tighter curl and less coat volume.', 'The Selkirk Rex originated in Montana, USA in 1987, with a litter born to a rescued cat.', '[3] The only unusually coated kitten in the litter was ultimately placed with breeder, Jeri Newman, who named her Miss DiPesto (after a curly-haired character in the TV series Moonlighting played by Allyce Beasley).', 'This foundation cat was bred to a black Persian tomcat, producing three Selkirk Rex and three straight-haired kittens.', 'This demonstrated that the gene had an autosomal dominant mode of inheritance.']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# Example usage:\n",
    "sentences = sentence_tokenize(text_data)\n",
    "print(sentences[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0ef044-367d-40f4-981c-04341c43256c",
   "metadata": {},
   "source": [
    "# 3. Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14ce119-69ac-4be5-bd30-e8daa31f1f67",
   "metadata": {},
   "source": [
    " **NLTK Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8a3cb493-78f7-495c-ada7-60ac66904260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Selkirk', 'Rex', 'is', 'a', 'breed', 'of', 'cat', 'with', 'highly', 'curled', 'hair', '1', 'The', 'Selkirk', 'Rex', 'is', 'distinct', 'from', 'all', 'other', 'Rex', 'breed', 'Unlike', 'the', 'Devon', 'Rex', 'and', 'Cornish', 'Rex', 'the', 'hair', 'is', 'of', 'normal', 'length', 'and', 'not', 'partly', 'missing', 'There', 'are', 'longhair', 'and', 'shorthair', 'variety', '2', 'It', 'differs', 'from', 'the', 'LaPerm', 'in', 'that', 'the', 'Selkirk', 'Rex', 'coat', 'is', 'plusher', 'and', 'thicker', 'While', 'the', 'LaPerm', 'gene', 'is', 'a', 'simple', 'dominant', 'the', 'Selkirk', 'gene', 'Se', 'act', 'a', 'an', 'incomplete', 'dominant', 'incompletely', 'dominant', 'allele', 'pair', 'produce', 'three', 'possible', 'genotype', 'and', 'phenotype', 'heterozygous', 'cat', 'Sese', 'may', 'have', 'a', 'fuller', 'coat', 'that', 'is', 'preferred']\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    # Initialize WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Lemmatize the text\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text) if word not in string.punctuation]\n",
    "\n",
    "    return lemmatized_words\n",
    "\n",
    "lemmatized_words = lemmatize_text(text_data)\n",
    "print(lemmatized_words[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d911984-3cc9-4c26-88a4-99cedff5d425",
   "metadata": {},
   "source": [
    "**Stemming using NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "84691e0f-b2aa-4136-82b6-a7fff5bbd819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'selkirk', 'rex', 'is', 'a', 'breed', 'of', 'cat', 'with', 'highli', 'curl', 'hair', '1', 'the', 'selkirk', 'rex', 'is', 'distinct', 'from', 'all', 'other', 'rex', 'breed', 'unlik', 'the', 'devon', 'rex', 'and', 'cornish', 'rex', 'the', 'hair', 'is', 'of', 'normal', 'length', 'and', 'not', 'partli', 'miss', 'there', 'are', 'longhair', 'and', 'shorthair', 'varieti', '2', 'it', 'differ', 'from', 'the', 'laperm', 'in', 'that', 'the', 'selkirk', 'rex', 'coat', 'is', 'plusher', 'and', 'thicker', 'while', 'the', 'laperm', 'gene', 'is', 'a', 'simpl', 'domin', 'the', 'selkirk', 'gene', 'se', 'act', 'as', 'an', 'incomplet', 'domin', 'incomplet', 'domin', 'allel', 'pair', 'produc', 'three', 'possibl', 'genotyp', 'and', 'phenotyp', 'heterozyg', 'cat', 'sese', 'may', 'have', 'a', 'fuller', 'coat', 'that', 'is', 'prefer']\n"
     ]
    }
   ],
   "source": [
    "def stem_text_nltk(text):\n",
    "    # Initialize PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Stem the text\n",
    "    stemmed_words = [stemmer.stem(word) for word in word_tokenize(text) if word not in string.punctuation]\n",
    "\n",
    "    return stemmed_words\n",
    "\n",
    "stemmed_words = stem_text_nltk(text_data)\n",
    "print(stemmed_words[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a290695-7b58-4439-a7dd-0ec8f0783ef4",
   "metadata": {},
   "source": [
    "# 4. Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3a8d7430-6c2d-4f87-a4b2-917afe74f452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Selkirk', 'Rex', 'breed', 'cat', 'highly', 'curled', 'hair', '1', 'Selkirk', 'Rex', 'distinct', 'Rex', 'breeds', 'Unlike', 'Devon', 'Rex', 'Cornish', 'Rex', 'hair', 'normal', 'length', 'partly', 'missing', 'longhair', 'shorthair', 'varieties', '2', 'differs', 'LaPerm', 'Selkirk', 'Rex', 'coat', 'plusher', 'thicker', 'LaPerm', 'gene', 'simple', 'dominant', 'Selkirk', 'gene', 'Se', 'acts', 'incomplete', 'dominant', 'incompletely', 'dominant', 'allele', 'pairs', 'produce', 'three', 'possible', 'genotypes', 'phenotypes', 'heterozygous', 'cats', 'Sese', 'may', 'fuller', 'coat', 'preferred', 'show', 'ring', 'homozygous', 'cats', 'SeSe', 'may', 'tighter', 'curl', 'less', 'coat', 'volume', 'Selkirk', 'Rex', 'originated', 'Montana', 'USA', '1987', 'litter', 'born', 'rescued', 'cat', '3', 'unusually', 'coated', 'kitten', 'litter', 'ultimately', 'placed', 'breeder', 'Jeri', 'Newman', 'named', 'Miss', 'DiPesto', 'curly-haired', 'character', 'TV', 'series', 'Moonlighting', 'played']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords dataset\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    # Get the list of English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Filter out stop words from the token list\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = remove_stop_words(tokens_nltk)\n",
    "\n",
    "print(filtered_tokens[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb76d64-5c30-4c7a-aff9-aa268914bce7",
   "metadata": {},
   "source": [
    "# 5. feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ab0ea-6df6-4cce-adde-3705414d9d9e",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4fc6f087-8fa6-4198-b70c-3658cb5b4e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n",
      "['10' '1987' '1992' '1998' '2000' '2015' '21' 'abnormality' 'accepted'\n",
      " 'acts' 'actual' 'adwh' 'after' 'albinism' 'all' 'allele' 'allelic'\n",
      " 'allergens' 'allergic' 'allyce' 'also' 'american' 'amounts' 'an'\n",
      " 'ancestry' 'and' 'are' 'as' 'associated' 'association' 'australia'\n",
      " 'autosomal' 'back' 'bathing' 'be' 'beasley' 'been' 'bicolors' 'black'\n",
      " 'born' 'break' 'bred' 'breed' 'breeder' 'breeders' 'breeding' 'breeds'\n",
      " 'british' 'built' 'by' 'can' 'cardiomyopathy' 'cat' 'cats' 'cfa'\n",
      " 'character' 'characterized' 'chocolate' 'cinnamon' 'coat' 'coated'\n",
      " 'colors' 'copies' 'cornish' 'council' 'creases' 'crucial' 'cuddly' 'curl'\n",
      " 'curled' 'curls' 'curly' 'currently' 'defined' 'demonstrated' 'dense'\n",
      " 'dermatitis' 'designation' 'develop' 'developed' 'development' 'devon'\n",
      " 'differs' 'dipesto' 'discontinued' 'disease' 'diseases' 'disqualifiable'\n",
      " 'distinct' 'dominant' 'down' 'ducts' 'ear' 'ears' 'equal' 'except'\n",
      " 'excessive' 'exotic' 'extreme' 'extremely']\n",
      "------------------------------------------------------------\n",
      "Most Frequent Words: [('the', 63), ('of', 24), ('and', 23), ('in', 22), ('rex', 19), ('is', 19), ('selkirk', 15), ('to', 13), ('breed', 12), ('cat', 12)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define sample corpus of documents\n",
    "corpus = sentences\n",
    "# Create an instance of CountVectorizer with default settings\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the corpus and transform the corpus into a BOW matrix\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the BOW matrix\n",
    "print(bow_matrix.toarray())\n",
    "\n",
    "# Print the feature names (i.e., unique words) learned by the vectorizer\n",
    "print(vectorizer.get_feature_names_out()[:100])\n",
    "\n",
    "print(\"------------------------------------------------------------\")\n",
    "\n",
    "def get_most_frequent_words(X, vectorizer, top_n=10):\n",
    "    # Sum up the counts of each vocabulary word\n",
    "    word_counts = np.asarray(X.sum(axis=0)).flatten()\n",
    "    \n",
    "    # Get the indices that would sort the array in descending order\n",
    "    sorted_indices = np.argsort(word_counts)[::-1]\n",
    "    \n",
    "    # Get the feature names (words)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get the top N most frequent words\n",
    "    most_frequent_words = [(feature_names[idx], word_counts[idx]) for idx in sorted_indices[:top_n]]\n",
    "    \n",
    "    return most_frequent_words\n",
    "\n",
    "# Get the most frequent words\n",
    "most_frequent_words = get_most_frequent_words(bow_matrix, vectorizer, top_n=10)\n",
    "print(\"Most Frequent Words:\", most_frequent_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb13eb-26c9-4e33-a922-d8deac2829a7",
   "metadata": {},
   "source": [
    "### TF-IDF - words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2bfb09c9-fca2-4ade-89b7-af2e9c08b643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Feature Names: ['10' '1987' '1992' '1998' '2000' '2015' '21' 'abnormality' 'accepted'\n",
      " 'acts' 'actual' 'adwh' 'albinism' 'allele' 'allelic' 'allergens'\n",
      " 'allergic' 'allyce' 'also' 'american' 'amounts' 'ancestry' 'associated'\n",
      " 'association' 'australia' 'autosomal' 'back' 'bathing' 'beasley'\n",
      " 'bicolors' 'black' 'born' 'break' 'bred' 'breed' 'breeder' 'breeders'\n",
      " 'breeding' 'breeds' 'british' 'built' 'cardiomyopathy' 'cat' 'cats' 'cfa'\n",
      " 'character' 'characterized' 'chocolate' 'cinnamon' 'coat' 'coated'\n",
      " 'colors' 'copies' 'cornish' 'council' 'creases' 'crucial' 'cuddly' 'curl'\n",
      " 'curled' 'curls' 'curly' 'currently' 'defined' 'demonstrated' 'dense'\n",
      " 'dermatitis' 'designation' 'develop' 'developed' 'development' 'devon'\n",
      " 'differs' 'dipesto' 'discontinued' 'disease' 'diseases' 'disqualifiable'\n",
      " 'distinct' 'dominant' 'ducts' 'ear' 'ears' 'equal' 'except' 'excessive'\n",
      " 'exotic' 'extreme' 'extremely' 'eyes' 'face' 'fanciers' 'fancy' 'fault'\n",
      " 'feel' 'first' 'follicle' 'forming' 'found' 'foundation']\n",
      "TF-IDF Features:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "First row of the TF-IDF Features:\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tfidf_extraction(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X, vectorizer\n",
    "\n",
    "# Example usage:\n",
    "X_tfidf, vectorizer_tfidf = tfidf_extraction(filtered_tokens)\n",
    "print(\"TF-IDF Feature Names:\", vectorizer_tfidf.get_feature_names_out()[:100])\n",
    "print(\"TF-IDF Features:\\n\", X_tfidf.toarray())\n",
    "# Print the first row of the TF-IDF feature matrix\n",
    "print(\"First row of the TF-IDF Features:\\n\", X_tfidf.toarray()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f590029-f7c0-47ea-be72-a2dd72c0073d",
   "metadata": {},
   "source": [
    "### TF-IDF - sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "90cedb87-8b60-4b9a-aed1-2a80eca13b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Feature Names: ['10' '1987' '1992' '1998' '2000' '2015' '21' 'abnormality' 'accepted'\n",
      " 'acts' 'actual' 'adwh' 'after' 'albinism' 'all' 'allele' 'allelic'\n",
      " 'allergens' 'allergic' 'allyce' 'also' 'american' 'amounts' 'an'\n",
      " 'ancestry' 'and' 'are' 'as' 'associated' 'association' 'australia'\n",
      " 'autosomal' 'back' 'bathing' 'be' 'beasley' 'been' 'bicolors' 'black'\n",
      " 'born' 'break' 'bred' 'breed' 'breeder' 'breeders' 'breeding' 'breeds'\n",
      " 'british' 'built' 'by' 'can' 'cardiomyopathy' 'cat' 'cats' 'cfa'\n",
      " 'character' 'characterized' 'chocolate' 'cinnamon' 'coat' 'coated'\n",
      " 'colors' 'copies' 'cornish' 'council' 'creases' 'crucial' 'cuddly' 'curl'\n",
      " 'curled' 'curls' 'curly' 'currently' 'defined' 'demonstrated' 'dense'\n",
      " 'dermatitis' 'designation' 'develop' 'developed' 'development' 'devon'\n",
      " 'differs' 'dipesto' 'discontinued' 'disease' 'diseases' 'disqualifiable'\n",
      " 'distinct' 'dominant' 'down' 'ducts' 'ear' 'ears' 'equal' 'except'\n",
      " 'excessive' 'exotic' 'extreme' 'extremely' 'eyes' 'face' 'fanciers'\n",
      " 'fancy' 'fault' 'feel' 'first' 'follicle' 'for' 'forming' 'found'\n",
      " 'foundation' 'frequency' 'from' 'front' 'fuller' 'fur' 'gccf' 'gene'\n",
      " 'genetically' 'genotypes' 'governing' 'greasiness' 'had' 'hair' 'haired'\n",
      " 'hairless' 'half' 'has' 'have' 'head' 'health' 'her' 'heterozygous'\n",
      " 'high' 'highly' 'himalayans' 'homozygous' 'hr' 'hypertrophic'\n",
      " 'identified' 'impact' 'in' 'including' 'incomplete' 'incompletely'\n",
      " 'increased' 'increasing' 'inheritance' 'inherited' 'inside'\n",
      " 'international' 'irritation' 'is' 'it' 'its' 'jeri' 'keratinization'\n",
      " 'kidney' 'kinking' 'kitten' 'kittens' 'known' 'krt71' 'krt71hr' 'krt71re'\n",
      " 'krt71sadre' 'laid' 'laperm' 'large' 'length' 'lengths' 'less' 'like'\n",
      " 'lilac' 'litter' 'locus' 'long' 'longhair' 'look' 'loose' 'lot' 'making'\n",
      " 'may' 'medium' 'might' 'minimize' 'mink' 'miss' 'missing' 'mode'\n",
      " 'montana' 'moonlighting' 'mutations' 'muzzle' 'named' 'nature'\n",
      " 'necessary' 'newman' 'no' 'normal' 'not' 'now' 'occur' 'of' 'on' 'only'\n",
      " 'or' 'originated' 'other' 'outcross' 'outcrosses' 'pairs' 'parents'\n",
      " 'partly' 'persian' 'persians' 'person' 'phenotype' 'phenotypes' 'placed'\n",
      " 'played' 'playfulness' 'plusher' 'pointed' 'polycystic' 'possible'\n",
      " 'preferred' 'prevent' 'problems' 'produce' 'producing' 'production'\n",
      " 'propensity' 'proper' 'qualities' 're' 'recommended' 'reduced' 'reflects'\n",
      " 'registered' 'requiring' 'rescued' 'reserved' 'responsible' 'result'\n",
      " 'resulting' 'rex' 'rexoid' 'ring' 'robust' 'round' 'rounded' 'run'\n",
      " 'sadre' 'scheduled' 'screen' 'se' 'selkirk' 'sepia' 'series' 'sese'\n",
      " 'shafts' 'shedding' 'short' 'shorthair' 'shorthairs' 'show' 'silver'\n",
      " 'similar' 'simple' 'sized' 'smoke' 'soft' 'solidly' 'specific' 'sphynx'\n",
      " 'splice' 'stepfather' 'straight' 'structure' 'suggested' 'tear'\n",
      " 'temperament' 'tendency' 'tested' 'that' 'the' 'their' 'there' 'these'\n",
      " 'they' 'thicker' 'this' 'those' 'three' 'tica' 'tighter' 'tightly' 'to'\n",
      " 'tomcat' 'towards' 'trace' 'tv' 'two' 'uk' 'ultimately' 'unlike'\n",
      " 'unstructured' 'unusually' 'usa' 'used' 'variant' 'varieties' 'very'\n",
      " 'volume' 'was' 'wax' 'where' 'while' 'who' 'whose' 'width' 'with'\n",
      " 'woolly']\n",
      "TF-IDF Features:\n",
      " [[0.         0.         0.         ... 0.         0.2895773  0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.19831728 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tfidf_extraction(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X, vectorizer\n",
    "\n",
    "# Example usage:\n",
    "X_tfidf, vectorizer_tfidf = tfidf_extraction(sentences)\n",
    "print(\"TF-IDF Feature Names:\", vectorizer_tfidf.get_feature_names_out())\n",
    "print(\"TF-IDF Features:\\n\", X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5740dd1-5e12-4e9e-b72b-610b0b70cd51",
   "metadata": {},
   "source": [
    "### Word Embedding by Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "27b5cd88-c95b-423e-8684-1f78317d4b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'cat': [-2.6747298e-03  8.1716133e-03 -9.8687407e-05  3.9941417e-03\n",
      " -8.5165142e-04 -4.6826019e-03  5.2845883e-03  9.3835266e-03\n",
      " -4.7295033e-03  5.5750180e-03 -4.7772001e-03  4.3378030e-03\n",
      "  5.7801306e-03 -1.0127712e-03  4.7383918e-03 -4.5551695e-03\n",
      " -8.4149297e-03 -8.0721816e-03 -1.0595262e-03  8.0520473e-03\n",
      " -7.4484632e-03 -8.8447449e-04  4.0806402e-03 -5.9527336e-03\n",
      "  4.4323276e-03 -3.8808680e-03  6.1732149e-03  1.5843904e-03\n",
      "  9.3814492e-04 -6.4645447e-03 -7.3588250e-04  7.1322848e-03\n",
      " -9.6095204e-03  5.1703895e-03 -1.4475227e-04  4.3892586e-03\n",
      "  3.2659601e-03 -1.3581396e-03 -3.8266564e-03  2.5461756e-03\n",
      " -7.1955156e-03  1.6819584e-03 -8.1061069e-03  2.9969311e-03\n",
      "  2.5749051e-03 -8.3111143e-03  5.1497580e-03 -1.6838526e-03\n",
      "  5.7377289e-03 -9.1677178e-03 -6.4108707e-03 -1.2018442e-04\n",
      " -6.2354971e-03 -3.4027768e-03  3.2119798e-03 -7.1095168e-03\n",
      "  3.5540580e-03 -7.9319421e-03 -6.4962972e-03  1.7528903e-03\n",
      "  5.8696638e-03 -6.5881405e-03 -4.0001450e-03  8.5024023e-03\n",
      "  9.7936736e-03  1.6212225e-03 -1.0592378e-03 -3.0626047e-03\n",
      "  2.2490383e-03  1.8183088e-03 -4.1049398e-03 -9.5439227e-03\n",
      " -6.9700349e-03  9.1711832e-03 -1.6369021e-03 -3.5393238e-04\n",
      " -6.5784930e-04  5.6547034e-03 -8.7008690e-03 -8.3454000e-03\n",
      " -4.4830563e-03 -2.6683451e-04  3.4224796e-03 -1.8586039e-04\n",
      " -1.1766386e-03  8.7565295e-03 -6.7796218e-03  1.4345610e-03\n",
      "  8.4489229e-04 -5.3021312e-04  6.1519481e-03 -4.6604872e-03\n",
      "  9.0754572e-03 -3.3686208e-03  9.0008858e-04  4.1344762e-04\n",
      "  6.9955573e-03 -1.2217712e-03 -4.3088212e-03 -9.5677590e-03]\n",
      "Most similar words to 'cat': [('association', 0.16580359637737274), ('breed', 0.10437854379415512), ('shorthair', 0.06660658866167068), ('hair', 0.0630192682147026), ('rex', 0.027065321803092957), ('selkirk', -0.0029750012326985598), ('gene', -0.008570496924221516), ('dominant', -0.014390558004379272), ('cats', -0.07193383574485779), ('breeds', -0.14684103429317474)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize the corpus\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in filtered_tokens]\n",
    "\n",
    "# Define corpus\n",
    "corpus = tokenized_corpus\n",
    "\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(\n",
    "    sentences=corpus,      # The corpus to train the model on\n",
    "    vector_size=100,       # The size of the word vectors to be learned\n",
    "    window=5,              # The size of the window of words to be considered\n",
    "    min_count=5,           # The minimum frequency required for a word to be included in the vocabulary\n",
    "    sg=0,                  # 0 for CBOW, 1 for skip-gram\n",
    "    negative=5,            # The number of negative samples to use for negative sampling\n",
    "    ns_exponent=0.75,      # The exponent used to shape the negative sampling distribution\n",
    "    alpha=0.03,            # The initial learning rate\n",
    "    min_alpha=0.0007,      # The minimum learning rate to which the learning rate will be linearly reduced\n",
    "    epochs=30,             # The number of epochs (iterations) over the corpus\n",
    "    workers=4,             # The number of worker threads to use for training the model\n",
    "    seed=42,               # The seed for the random number generator\n",
    "    max_vocab_size=None    # The maximum vocabulary size (None means no limit)\n",
    ")\n",
    "\n",
    "# Get the vector representation of a word\n",
    "vector = model.wv['cat']\n",
    "\n",
    "# Find the most similar words to a given word\n",
    "similar_words = model.wv.most_similar('cat')\n",
    "\n",
    "# Print the vector and similar words\n",
    "print(\"Vector for 'cat':\", vector)\n",
    "print(\"Most similar words to 'cat':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e2cd5a-fa4a-411d-aa8e-129cdf4f4fc5",
   "metadata": {},
   "source": [
    "## 6. What is GloVe?\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm for obtaining vector representations for words. It combines the advantages of both the global matrix factorization and local context window methods to generate word embeddings. Developed by researchers at Stanford, GloVe aims to capture the global statistical information of a corpus while preserving the contextual information of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5e64b925-e1a5-4211-bbed-2123f995c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400001 words loaded!\n",
      "GloVe vector for 'cat':\n",
      " [ 0.23088    0.28283    0.6318    -0.59411   -0.58599    0.63255\n",
      "  0.24402   -0.14108    0.060815  -0.7898    -0.29102    0.14287\n",
      "  0.72274    0.20428    0.1407     0.98757    0.52533    0.097456\n",
      "  0.8822     0.51221    0.40204    0.21169   -0.013109  -0.71616\n",
      "  0.55387    1.1452    -0.88044   -0.50216   -0.22814    0.023885\n",
      "  0.1072     0.083739   0.55015    0.58479    0.75816    0.45706\n",
      " -0.28001    0.25225    0.68965   -0.60972    0.19578    0.044209\n",
      " -0.31136   -0.68826   -0.22721    0.46185   -0.77162    0.10208\n",
      "  0.55636    0.067417  -0.57207    0.23735    0.4717     0.82765\n",
      " -0.29263   -1.3422    -0.099277   0.28139    0.41604    0.10583\n",
      "  0.62203    0.89496   -0.23446    0.51349    0.99379    1.1846\n",
      " -0.16364    0.20653    0.73854    0.24059   -0.96473    0.13481\n",
      " -0.0072484  0.33016   -0.12365    0.27191   -0.40951    0.021909\n",
      " -0.6069     0.40755    0.19566   -0.41802    0.18636   -0.032652\n",
      " -0.78571   -0.13847    0.044007  -0.084423   0.04911    0.24104\n",
      "  0.45273   -0.18682    0.46182    0.089068  -0.18185   -0.01523\n",
      " -0.7368    -0.14532    0.15104   -0.71493  ]\n",
      "Embeddings for the first document:\n",
      " [array([-0.10605  , -0.89547  ,  0.67608  ,  0.049406 ,  0.19306  ,\n",
      "       -0.26762  , -0.24872  , -0.28142  , -0.50555  ,  0.063848 ,\n",
      "       -0.0080122,  0.69941  , -0.58007  , -0.28362  ,  0.16356  ,\n",
      "       -0.44925  ,  0.74704  ,  0.25338  , -0.67655  , -0.14949  ,\n",
      "       -0.67994  , -0.059885 ,  0.45123  , -0.20916  ,  0.13042  ,\n",
      "        0.24225  , -0.50005  ,  0.28932  , -0.16202  ,  0.29029  ,\n",
      "       -0.2633   , -0.19073  , -0.15683  , -0.47699  ,  0.72972  ,\n",
      "       -0.13603  , -0.19288  ,  0.41019  ,  0.3582   , -0.38344  ,\n",
      "        0.4936   ,  0.058492 , -0.060375 , -0.16751  , -0.6226   ,\n",
      "        0.63282  ,  1.0385   , -0.10795  ,  0.54924  ,  1.629    ,\n",
      "       -0.11679  , -0.091544 ,  0.46417  , -0.33474  , -0.21717  ,\n",
      "       -0.083942 ,  0.45951  , -0.45304  , -0.63833  , -0.45894  ,\n",
      "        0.65939  , -0.047998 , -0.081569 ,  0.2781   ,  0.33221  ,\n",
      "       -0.052348 , -0.17143  , -0.83128  ,  0.33674  ,  0.46746  ,\n",
      "        0.033861 ,  0.0096777,  0.032323 , -0.59228  , -0.31852  ,\n",
      "       -0.086299 ,  0.55374  , -0.024352 , -0.16791  ,  0.38594  ,\n",
      "        0.031293 ,  0.018448 ,  0.58181  , -0.21793  ,  0.37512  ,\n",
      "        0.19259  , -0.092357 ,  0.10728  ,  0.26446  ,  0.95077  ,\n",
      "       -0.22926  , -0.35309  , -0.83405  ,  0.01582  , -0.89573  ,\n",
      "       -0.35811  , -0.44054  ,  0.41379  , -0.6028   , -0.21015  ])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load GloVe vectors\n",
    "def load_glove_model(glove_file):\n",
    "    print(\"Loading Glove Model\")\n",
    "    glove_model = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array([float(val) for val in split_line[1:]])\n",
    "            glove_model[word] = embedding\n",
    "    print(f\"Done. {len(glove_model)} words loaded!\")\n",
    "    return glove_model\n",
    "\n",
    "# Path to the GloVe file \n",
    "glove_file = r\"C:\\Users\\USER\\Downloads\\glove.6B\\glove.6B.100d.txt\"\n",
    "\n",
    "# Load the GloVe model\n",
    "glove_model = load_glove_model(glove_file)\n",
    "\n",
    "# Function to get the GloVe embedding for a word\n",
    "def get_glove_embedding(word, glove_model):\n",
    "    return glove_model.get(word, np.zeros(100))\n",
    "\n",
    "# Example usage\n",
    "word = 'cat'\n",
    "word_embedding = get_glove_embedding(word, glove_model)\n",
    "print(f\"GloVe vector for '{word}':\\n\", word_embedding)\n",
    "\n",
    "# Apply GloVe embeddings to the tokenized corpus\n",
    "corpus_embeddings = []\n",
    "for doc in corpus:\n",
    "    doc_embeddings = [get_glove_embedding(word, glove_model) for word in doc]\n",
    "    corpus_embeddings.append(doc_embeddings)\n",
    "\n",
    "# Print the embeddings for the first document\n",
    "print(f\"Embeddings for the first document:\\n {corpus_embeddings[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef9e61-aac3-467f-9b19-ab8c1e1794f4",
   "metadata": {},
   "source": [
    "### **Explanation of the Results**\n",
    "\n",
    "**GloVe Vector for \"cat\"**\n",
    "- Vector Representation: The array represents the word \"cat\" in a 100-dimensional space. Each value in the vector encodes some aspect of the word's meaning and relationships with other words.\n",
    "- Semantic Information: Vectors that are close in this high-dimensional space correspond to words that are semantically similar. For example, the vector for \"cat\" will be close to vectors for \"dog,\" \"pet,\" or \"animal.\"\n",
    "\n",
    "**Document Embeddings:**\n",
    "\n",
    "- The document embedding is obtained by averaging (a;though this is not the only method) the vectors of all words in the document that have corresponding GloVe vectors.\n",
    "- This approach creates a single vector that represents the entire document, making it suitable for tasks like document classification, clustering, and similarity analysis.\n",
    "- The agregated vector covers the overall semantic content of the text: the meaning, theme and semantic nuances based on the words the text contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "bd5a078c-f09f-466c-af0f-7750b65c86c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:  a bird chased the dog in the garden\n",
      "Parse table: \n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|    | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   |\n",
      "+====+=====+=====+=====+=====+=====+=====+=====+=====+\n",
      "|  1 | Det | NP  |     |     | S   |     |     | S   |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  2 |     | N   |     |     |     |     |     |     |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  3 |     |     | V   |     | VP  |     |     | VP  |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  4 |     |     |     | Det | NP  |     |     |     |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  5 |     |     |     |     | N   |     |     |     |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  6 |     |     |     |     |     | P   |     | PP  |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  7 |     |     |     |     |     |     | Det | NP  |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  8 |     |     |     |     |     |     |     | N   |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n",
      "*********************************************************\n",
      "\n",
      "Input sentence:  A dog saw the bird\n",
      "Parse table: \n",
      "+----+-----+-----+-----+-----+-----+\n",
      "|    | 1   | 2   | 3   | 4   | 5   |\n",
      "+====+=====+=====+=====+=====+=====+\n",
      "|  1 | Det | NP  |     |     | S   |\n",
      "+----+-----+-----+-----+-----+-----+\n",
      "|  2 |     | N   |     |     |     |\n",
      "+----+-----+-----+-----+-----+-----+\n",
      "|  3 |     |     | V   |     | VP  |\n",
      "+----+-----+-----+-----+-----+-----+\n",
      "|  4 |     |     |     | Det | NP  |\n",
      "+----+-----+-----+-----+-----+-----+\n",
      "|  5 |     |     |     |     | N   |\n",
      "+----+-----+-----+-----+-----+-----+\n",
      "\n",
      "*********************************************************\n",
      "\n",
      "Input sentence:  The bird saw the dog in the garden\n",
      "Parse table: \n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|    | 1   | 2   | 3   | 4   | 5   | 6   | 7   | 8   |\n",
      "+====+=====+=====+=====+=====+=====+=====+=====+=====+\n",
      "|  1 | Det | NP  |     |     | S   |     |     | S   |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  2 |     | N   |     |     |     |     |     |     |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  3 |     |     | V   |     | VP  |     |     | VP  |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  4 |     |     |     | Det | NP  |     |     |     |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  5 |     |     |     |     | N   |     |     |     |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  6 |     |     |     |     |     | P   |     | PP  |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  7 |     |     |     |     |     |     | Det | NP  |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "|  8 |     |     |     |     |     |     |     | N   |\n",
      "+----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
      "\n",
      "*********************************************************\n",
      "\n",
      "Input sentence:  a dog played in the garden\n",
      "Parse table: \n",
      "+----+-----+-----+-----+-----+-----+-----+\n",
      "|    | 1   | 2   | 3   | 4   | 5   | 6   |\n",
      "+====+=====+=====+=====+=====+=====+=====+\n",
      "|  1 | Det | NP  |     |     |     | S   |\n",
      "+----+-----+-----+-----+-----+-----+-----+\n",
      "|  2 |     | N   |     |     |     |     |\n",
      "+----+-----+-----+-----+-----+-----+-----+\n",
      "|  3 |     |     | V   |     |     | VP  |\n",
      "+----+-----+-----+-----+-----+-----+-----+\n",
      "|  4 |     |     |     | P   |     | PP  |\n",
      "+----+-----+-----+-----+-----+-----+-----+\n",
      "|  5 |     |     |     |     | Det | NP  |\n",
      "+----+-----+-----+-----+-----+-----+-----+\n",
      "|  6 |     |     |     |     |     | N   |\n",
      "+----+-----+-----+-----+-----+-----+-----+\n",
      "\n",
      "*********************************************************\n",
      "\n",
      "Input sentence:  a dog played with a bird\n",
      "Parse table: \n",
      "+----+-----+-----+-----+-----+-----+-----+\n",
      "|    | 1   | 2   | 3   | 4   | 5   | 6   |\n",
      "+====+=====+=====+=====+=====+=====+=====+\n",
      "|  1 | Det | NP  |     |     |     | S   |\n",
      "+----+-----+-----+-----+-----+-----+-----+\n",
      "|  2 |     | N   |     |     |     |     |\n",
      "+----+-----+-----+-----+-----+-----+-----+\n",
      "|  3 |     |     | V   |     |     | VP  |\n",
      "+----+-----+-----+-----+-----+-----+-----+\n",
      "|  4 |     |     |     | P   |     | PP  |\n",
      "+----+-----+-----+-----+-----+-----+-----+\n",
      "|  5 |     |     |     |     | Det | NP  |\n",
      "+----+-----+-----+-----+-----+-----+-----+\n",
      "|  6 |     |     |     |     |     | N   |\n",
      "+----+-----+-----+-----+-----+-----+-----+\n",
      "\n",
      "*********************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cyk_parse(sentence, grammar):\n",
    "    tokens = sentence.lower().split()\n",
    "    n = len(tokens)\n",
    "    table = [[set() for _ in range(n+1)] for _ in range(n+1)]\n",
    "\n",
    "    # Initialization\n",
    "    for i in range(1, n+1):\n",
    "        for rule in grammar:\n",
    "            if len(rule) == 2 and rule[1] == tokens[i-1]:\n",
    "                table[i][i].add(rule[0])\n",
    "\n",
    "    # Rule Application\n",
    "    for length in range(2, n+1):\n",
    "        for i in range(1, n-length+2):\n",
    "            j = i + length - 1\n",
    "            for k in range(i, j):\n",
    "                for rule in grammar:\n",
    "                    if len(rule) == 3:\n",
    "                        if rule[1] in table[i][k] and rule[2] in table[k+1][j]:\n",
    "                            table[i][j].add(rule[0])\n",
    "                    elif len(rule) == 4:\n",
    "                        for m in range(k+1, j):\n",
    "                            if rule[1] in table[i][k] and rule[2] in table[k+1][m] and rule[3] in table[m+1][j]:\n",
    "                                table[i][j].add(rule[0])\n",
    "\n",
    "    # Check if the start symbol is in the top-right cell\n",
    "    if 'S' in table[1][n]:\n",
    "        return True, table\n",
    "    else:\n",
    "        return False, table\n",
    "\n",
    "# Define the updated context-free grammar in CNF\n",
    "grammar = [\n",
    "    ('S', 'NP', 'VP'),\n",
    "    ('VP', 'V', 'NP'),\n",
    "    ('VP', 'V', 'PP'),\n",
    "    ('VP', 'V', 'NP', 'PP'),\n",
    "    ('NP', 'Det', 'N'),\n",
    "    ('NP', 'N'),\n",
    "    ('PP', 'P', 'NP'),\n",
    "    ('Det', 'the'),\n",
    "    ('Det', 'a'),\n",
    "    ('N', 'dog'),\n",
    "    ('N', 'bird'),\n",
    "    ('N', 'garden'),\n",
    "    ('V', 'saw'),\n",
    "    ('V', 'chased'),\n",
    "    ('V', 'played'),\n",
    "    ('P', 'in'),\n",
    "    ('P', 'with')\n",
    "]\n",
    "\n",
    "# List of input sentences to be parsed\n",
    "sentences = [\n",
    "    \"a bird chased the dog in the garden\",\n",
    "    \"A dog saw the bird\",\n",
    "    \"The bird saw the dog in the garden\",\n",
    "    \"a dog played in the garden\",\n",
    "    \"a dog played with a bird\"\n",
    "]\n",
    "# Call the CYK parser\n",
    "for sentence in sentences:\n",
    "    parsed, table = cyk_parse(sentence, grammar)\n",
    "    \n",
    "    # Print the parse table and whether the sentence was parsed or not\n",
    "    if parsed:\n",
    "        print(\"Input sentence: \", sentence)\n",
    "        print(\"Parse table: \")\n",
    "       \n",
    "        n = len(table) - 1\n",
    "        headers = [''] + [str(i) for i in range(1, n + 1)]\n",
    "        rows = []\n",
    "        for i in range(1, n + 1):\n",
    "            row = [str(i)]\n",
    "            for j in range(1, n + 1):\n",
    "                row.append(\", \".join(sorted(table[i][j])))\n",
    "            rows.append(row)\n",
    "        print(tabulate(rows, headers=headers, tablefmt='grid'))\n",
    "        print(\"\\n*********************************************************\\n\")\n",
    "    else:\n",
    "        print(\"Input sentence: \", sentence)\n",
    "        print(\"Sentence not parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75083ff-cfb5-4304-885c-9831bd847b07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
